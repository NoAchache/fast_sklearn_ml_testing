""" Training steps called by the training pipeline """

from functools import partial
from pathlib import Path
from pprint import pprint
from typing import Callable, Literal

import numpy as np
import optuna
import pandas as pd
from dill.source import getname
from sklearn.model_selection import cross_val_predict

from src.config.models_metrics_plots_per_task import MODELS_METRICS_PLOTS_PER_TASK
from src.utils.utils import compute_metric, get_callable_from_name, plot_and_save


class TrainingSteps:
    """
    Contains methods related to the interactions with the model such as fitting, predicting, etc ...

    Parameters
    ----------
    config: Config dict to parametrize the training pipeline
    """

    def __init__(self, config: dict) -> None:
        self.config = config
        models_metrics_and_plots = MODELS_METRICS_PLOTS_PER_TASK[config["task"]]

        self.model_class = get_callable_from_name(
            config["model"], models_metrics_and_plots["models"]
        )
        self.model = self.model_class(**config["model_arguments"])
        self.prediction_method = models_metrics_and_plots["prediction_method"]
        self.metrics = models_metrics_and_plots["metrics"]
        self.plots = models_metrics_and_plots["plots"]

    def cross_validate(
        self, X_train: pd.DataFrame, y_true_train: pd.Series, logging: bool = False
    ) -> pd.DataFrame:
        """
        Cross-validate the input data with self.model

        Parameters
        ----------
        X_train: training features
        y_true_train: ground truth of the training features
        logging: Whether to display the logging

        Returns
        -------
        Cross-validated predictions
        """

        if logging:
            print("Starting cross validation")

        y_pred_cross_val = cross_val_predict(
            self.model,
            X_train,
            y_true_train,
            cv=self.config["cross_validation"]["folds_number"],
            method=self.prediction_method,
        )

        return self.postprocess_predictions(y_pred_cross_val)

    def hyperoptimization(
        self,
        X_train: pd.DataFrame,
        y_true_train: pd.Series,
        optimization_dict_function: Callable,
    ) -> None:
        """
        Hyperoptimize the model to find an optimized set of hyperparameters. The objective function
        self._hyperoptimization_objective is run at each evaluation. At the end of the method, the
        attribute self.model is re-created, with the hyperpameters found.

        Parameters
        ----------
        X_train: training features
        y_true_train: ground truth of the training features
        optimization_dict_function: Function returning a dict of hyperparameters to optimize, with the associated
        optimization ranges.
        """
        metric_name = self.config["hyperoptimization"]["metric"]
        metric_function = get_callable_from_name(metric_name, self.metrics)

        study = optuna.create_study(
            direction="minimize"
            if self.config["hyperoptimization"]["metric_decreases_when_optimized"]
            else "maximize"
        )
        study.optimize(
            partial(
                self._hyperoptimization_objective,
                optimization_dict_function=optimization_dict_function,
                X_train=X_train,
                y_true_train=y_true_train,
                metric_function=metric_function,
            ),
            n_trials=self.config["hyperoptimization"]["evaluations_number"],
            show_progress_bar=True,
        )

        print(
            f"Best hyperopt metric score: {study.best_trial.value}.\nOptimized"
            f" parameters: {study.best_trial.params}"
        )

        # Define new model with hyperoptimized parameters, which will be used for the inference phase of the pipeline
        self.model = self.model_class(**study.best_trial.params)

    def _hyperoptimization_objective(
        self,
        trial: optuna.Trial,
        X_train: pd.DataFrame,
        y_true_train: pd.Series,
        metric_function: Callable,
        optimization_dict_function: Callable,
    ) -> float:
        """
        Computes a metric score for an optuna optimization round. To do so, a new model is instantiated with a set
        of parameters defined by the trial and optimization_dict_function. The train set is then cross-validated to
        obtain predictions. The predictions are compared to the ground truth in the metric_function to obtain the
        metric.

        Parameters
        ----------
        trial: generated by the optuna study optimization
        X_train: training data
        y_true_train: ground truth of the training data
        metric_function: metric function, to be specified in the config yaml.`
        optimization_dict_function: Function returning a dict of hyperparameters to optimize, with the associated
        optimization ranges.
        """

        sampled_parameters = optimization_dict_function(trial)

        self.model = self.model_class(**sampled_parameters)
        y_pred_cross_val = self.cross_validate(X_train, y_true_train, logging=False)

        return compute_metric(
            metric_function,
            y_true_train,
            y_pred_cross_val,
        )

    def fit(
        self,
        X_train: pd.DataFrame,
        y_true_train: pd.Series,
    ) -> None:
        """
        Fits self.model

        Parameters
        ----------
        X_train: training data
        y_true_train: ground truth of the training data
        """

        print("Starting to fit model")
        self.model.fit(X_train, y_true_train)
        self.model.feature_names = list(X_train.columns)

    def get_predictions(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Infers the input data in self.model. The prediction method used depends on the task.

        Parameters
        ----------
        X: data to infer

        Returns
        -------
        Predictions for X
        """

        if self.prediction_method == "predict":
            y_pred = self.model.predict(X)
        elif self.prediction_method == "predict_proba":
            y_pred = self.model.predict_proba(X)
        else:
            raise ValueError("Unknown prediction method")

        return self.postprocess_predictions(y_pred)

    def compute_and_save_metrics_and_plots(
        self,
        X: pd.DataFrame,
        y_true,
        y_pred,
        experiment_dir: Path,
        predictions_type: Literal["cross_validation", "validation", "test"],
    ) -> None:
        """
        Computes successively the metrics and plots for a specific set. The metrics and plots are specified
        in src/config/models_metrics_plots_per_task.py.

        Parameters
        ----------
        X: features
        y_true: ground truth corresponding to the features
        y_pred: predictions corresponding to the features
        experiment_dir: directory in which the metrics and plots are saved
        predictions_type: Whether the predictions from cross_validation (on train set), validation set or test set
        """
        sub_experiment_dir = experiment_dir / predictions_type
        sub_experiment_dir.mkdir()

        metric_results = {
            getname(metric): compute_metric(
                metric,
                y_true,
                y_pred,
            )
            for metric in self.metrics
        }
        print(f"Metris obtained for {predictions_type}")
        pprint(metric_results)

        # pylint: disable=expression-not-assigned # no assignment needed for the below comprehension lists.

        with open(
            sub_experiment_dir / "metrics.txt", "w", encoding="utf-8"
        ) as metric_file:
            [
                metric_file.write(f"{metric_name}: {score}\n")
                for metric_name, score in metric_results.items()
            ]

        [
            plot_and_save(plot, self.model, X, y_true, y_pred, sub_experiment_dir)
            for plot in self.plots
        ]

    def postprocess_predictions(self, y_pred: np.ndarray) -> pd.DataFrame:
        """
        In the case of binary-predictions, only keeps the probability that the result is 1 (useful for later
        processing), which corresponds to the second column (y_pred[:, 1]) of the predictions.
        PS: The first column (y_pred[:, 0]) is the probability that the result is 0 and is equal to 1 - y_pred[:, 1].

        Parameters
        ----------
        y_pred: predictions to process

        Returns
        -------
        In the case of binary-predictions, the probability that the result is 1. Else, the same data as the input.
        """

        y_pred = (
            y_pred[:, 1] if self.config["task"] == "binary-classification" else y_pred
        )
        return pd.DataFrame(y_pred)
